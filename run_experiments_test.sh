MODEL_RUN_NAME="qwen_lr_5e3_4bs_16ga_18L_32q1kv_32headdim_wsd_test"
python -m MaxText.train src/MaxText/configs/base.yml model_name=qwen3-0.6b base_num_query_heads=32 base_num_kv_heads=1 ici_data_parallelism=8 tokenizer_path="HuggingFaceTB/SmolLM2-135M" run_name=$MODEL_RUN_NAME base_output_directory=/content/maxtext-outputs  dataset_type=hf hf_path='arrow' hf_data_dir='' hf_train_files='/kaggle/temp/fineweb-edu-sample-10bt/data-*-of-*.arrow'  eval_interval=50 hf_eval_split='' hf_eval_files='/kaggle/temp/fineweb-edu-sample-10bt-val/data-*-of-*.arrow' steps=500 per_device_batch_size=4 gradient_accumulation_steps=16 fused_mlp=True  sa_use_fused_bwd_kernel=True dtype=bfloat16 dtype_mm=bfloat16 weight_dtype=bfloat16 optimize_mesh_for_tpu_v6e=True scan_layers=False remat_policy=minimal_with_context max_target_length=1024 learning_rate=5e-3 eval_steps=80 eval_per_device_batch_size=64 generate_padding_batch_eval=True  grain_worker_count=16 grain_worker_count_eval=16 enable_checkpointing=True async_checkpointing=True checkpoint_period=250 packing=True
python upload_to_hf.py --folder_path "/content/maxtext-outputs/${MODEL_RUN_NAME}" --repo_id "ahycl/${MODEL_RUN_NAME}"
